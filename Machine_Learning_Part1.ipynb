{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating Mouse and Human Antibody Sequences using Protein Encoding and Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- How may we create a machine learning classifier that can tell apart two groups of proteins?\n",
    "\n",
    "- How may we improve upon the performance of a machine learning classifier that does not perform so well?\n",
    "\n",
    "- How may we check for overfitting in a trained machine learning classifier?\n",
    "\n",
    "- How may we pick the most relevant data points that are related to the class of a sample?\n",
    "\n",
    "- Can we also train a deep learning predictor to tell proteins apart?\n",
    "\n",
    "### Objectives:\n",
    "    \n",
    "- Understand how protein sequences can become readable to machine learning predictors\n",
    "\n",
    "- Practise machine learning optimisation techniques including GridSearchCV and dimensionality reduction\n",
    "\n",
    "- Check for overfitted data by testing with a totally naive dataset\n",
    "\n",
    "- Pracitse prediction by applying a deep learning model to the problem and evaluating its performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "If we want to generate a classifier that observes the differences between two groups of protein seequences, then we need to extract numerical information from our sequences. This is called encoding and can be done through a variety of ways inlcuding residue level encoding of each amino acid in your sequences with a 1x20 vector, representing the possibility of 20 amino acids at each residue. This is called One Hot Encoding, but often leads to a sparse dataset which is not suitable for machine learning tasks, and each sequence must be spaced out so they are of equal length. Instead, in this example we use the physiochemical properties that may be calculated from the sequence as our numeric information (ElAbd *et al.*, 2020). \n",
    "\n",
    "Antibodies are made up of two heavy and two light chains, however, the functional antigen-binding domains are Fv fragments at each fork of the \"Y\" shape. These Fv fragments are where the VH domain of a heavy chain and VL domain of a light chain interact and so much study has been dedicated to these regions. An antibody record is considered \"paired\" when both the VH and VL sequences of one antibody are known. This knowledge was rare in the past and came from studying individual antibodies, however, the advent of B-cell encapsulation and Next Generation Sequencing now allowed researchers to sequence a repertoire of paired antibodies (Rajan *et al.*, 2018).\n",
    "\n",
    "In this exercise, we will use a sample of 1000 Human and 1000 Mouse paired antibodies taken from the Observed Antibody Space (Olsen *et al.*, 2022) and develop a machine learning classifier to separate them. Firstly, we will use Propythia (Sequeira *et al.*, 2022) to generate our encodings from an input of Fasta formatted sequences. Secondly, we will split those encodings into training and test datasets for a selection of machine learning classifiers and plot our results. Finally we will try to improve our performance through principal component analysis (PCA), which also helps to visualise our dataset.\n",
    "\n",
    "### References\n",
    "\n",
    "ElAbd, H., Bromberg, Y., Hoarfrost, A., Lenz, T., Franke, A., & Wendorff, M. (2020). Amino acid encoding for deep learning applications. BMC Bioinformatics, 21(1), 235. https://doi.org/10.1186/s12859-020-03546-x\n",
    "\n",
    "Olsen, T. H., Boyles, F., & Deane, C. M. (2022). Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired\n",
    "and paired antibody sequences. Protein Science, 31(1), 141–146. https://doi.org/https://doi.org/10.1002/pro.4205\n",
    "\n",
    "Sequeira, A. M., Lousa, D., & Rocha, M. (2022). ProPythia: A Python package for protein classification based on machine and deep learning.\n",
    "Neurocomputing, 484, 172–182. https://doi.org/https://doi.org/10.1016/j.neucom.2021.07.102\n",
    "\n",
    "Rajan, S., Kierny, M. R., Mercer, A., Wu, J., Tovchigrechko, A., Wu, H., Dall′Acqua, W. F., Xiao, X., & Chowdhury, P. S. (2018). Recombinant\n",
    "human B cell repertoires enable screening for rare, specific, and natively paired antibodies. Communications Biology, 1(1), 5. https://doi.org/10.1038/s42003-017-0006-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Requirements\n",
    "The antibody encoding method we will be using is the Propythia program. Before starting we recommend installing it by copying and pasting the following command in your terminal/command line:\n",
    "\n",
    "`pip install propythia`\n",
    "\n",
    "The machine learning models that we will use are imported from the sklearn package. We also suggest installing this via the following command: \n",
    "\n",
    "`pip install sklearn`\n",
    "\n",
    "As this is a large package, rather than importing the whole package, it is better practise to just import the fuctions we require. As this usually leaves with a lot of import statements, we have broken them down what imports are required at what stage of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get Encodings###\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "sys.path.append('../src/')\n",
    "sys.path.append('')\n",
    "from propythia.sequence import ReadSequence\n",
    "sequence=ReadSequence()\n",
    "from propythia.descriptors import Descriptor\n",
    "\n",
    "\n",
    "##Data preparation###\n",
    "from sklearn.utils import check_random_state, shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from numpy import pi, linspace, cos, sin, append, ones, zeros, hstack, vstack, intp\n",
    "from numpy import mgrid, linspace, c_, arange, mean, array\n",
    "from numpy.random import uniform, seed\n",
    "\n",
    "\n",
    "##Machine Learning Models###\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "\n",
    "##Plotting Results###\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.pyplot import subplots, axes, scatter, xticks\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "##Model Optimisation###\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Encoded Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we input our fasta file and split the entries into VH and VL sequences. We put each set of sequecnes through the Propythia encoder a dataframe of numerical information for both VH and VL sequences. There are 4000 records in the fasta file representing 2000 paired antibodies: 1000 human and 1000 mouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Propythia Command to get encodings###\n",
    "def get_descriptors(protein):\n",
    "    ##This will retrieve a selection of encodings that are not dependent on the sequence length##\n",
    "    test= protein.adaptable([3,4,5,6,7,8,9,10,11,12,13,14,17,18,19,20,21])\n",
    "    return(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_dataset(fasta):\n",
    "    VH_sequences = []\n",
    "    VL_sequences = []\n",
    "    with open(fasta, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] == \">\":\n",
    "                if \"_VH\" in line:\n",
    "                    sequence_to_add = f.readline().strip()\n",
    "                    VH_sequences.append(sequence_to_add)\n",
    "                elif \"_VL\" in line:\n",
    "                    sequence_to_add = f.readline().strip()\n",
    "                    VL_sequences.append(sequence_to_add)\n",
    "\n",
    "    print(len(VH_sequences),len(VL_sequences))\n",
    "    if len(VH_sequences) == len(VL_sequences):\n",
    "        VH_dataframe = pd.DataFrame()\n",
    "        VL_dataframe = pd.DataFrame()\n",
    "        for i in range(len(VH_sequences)):\n",
    "            ps_string=sequence.read_protein_sequence(VH_sequences[i])\n",
    "            protein = Descriptor(ps_string)\n",
    "            descriptors = get_descriptors(protein)\n",
    "            #VH_dataframe.loc[len(VH_dataframe)] = descriptors\n",
    "            VH_dataframe = VH_dataframe._append(descriptors, ignore_index=True)\n",
    "        print(\"VH_data\", VH_dataframe.shape)\n",
    "        for i in range(len(VL_sequences)):\n",
    "            ps_string=sequence.read_protein_sequence(VL_sequences[i])\n",
    "            protein = Descriptor(ps_string)\n",
    "            descriptors = get_descriptors(protein)\n",
    "            #VL_dataframe.loc[len(VL_dataframe)] = descriptors\n",
    "            VL_dataframe = VL_dataframe._append(descriptors, ignore_index=True)\n",
    "        print(\"VL_data\", VL_dataframe.shape)\n",
    "    # Now we join these two dataframes together so that each sample now has information about its VH and VL sequence.\n",
    "    VH_dataframe_suffix = VH_dataframe.add_suffix('_VH')\n",
    "    VL_dataframe_suffix = VL_dataframe.add_suffix('_VL')\n",
    "    joined_dataframe_VH_VL =  VH_dataframe_suffix.join(VL_dataframe_suffix)\n",
    "    return(joined_dataframe_VH_VL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input Fasta and Run Dataset###\n",
    "input_fasta = './HumanMouseOAS_VH_VL_paired_data.faa'\n",
    "\n",
    "#joined_dataframe_VH_VL = Get_dataset(input_fasta)\n",
    "\n",
    "#Optionally save dataframe as a CSV to simply reload it in future\n",
    "#joined_dataframe_VH_VL.to_csv('./HumanMouseOAS_VH_VL_paired_data.faa_Full_descriptors')#\n",
    "joined_dataframe_VH_VL = read_csv('./HumanMouseOAS_VH_VL_paired_data.faa_Full_descriptors.csv', header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "- These encodings used with Propythia were selected to reduce the time taken to run. Retry the encoding step and experiment with the protein.adaptable([3,4,5,6,7,8,9,10,11,12,13,14,17,18,19,20,21]) array.\n",
    "\n",
    "- NB Propythia accepts numbers 0-40 however we avoid 1, 2 and 37 as these produce outputs of differing length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our encodings, we need to prepare our labels. As our input was ordered 1000 Human antibodies and 1000 Mouse antibodies we can simply make a list showing only these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "#Prepare training data and labels\n",
    "labels1 = 1000*[1] ##Human antibodies will be class 1\n",
    "labels2 = 1000*[0] ## Mouse antibodies will be class 0\n",
    "labels = labels1+labels2\n",
    "y=labels\n",
    "print(len(y))\n",
    "##Mouse ==1, Human == 0\n",
    "\n",
    "dataset = joined_dataframe_VH_VL\n",
    "dataset=dataset.loc[:, dataset.columns != 'Unnamed: 0']\n",
    "print(dataset.shape) ##Just to check that you have an equal number of labels to the number of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets, we may not split them into training datasets for fitting our classifiers to and training datasets to verify their effectiveness as predictors. Usually 70/30 or 80/20 split is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=.3, random_state=RANDOM_SEED, shuffle=True)\n",
    "num_rows, num_cols = dataset.shape\n",
    "print(\"Training set size: \", X_train.shape, \"       Test set size: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating our data with Machine Learning Classifiers\n",
    "Here is our list of classifiers that we will loop through to see which is the best at clustering our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "RANDOM_SEED=42\n",
    "\n",
    "\n",
    "\n",
    "#    'KNeighbours': KNeighborsClassifier(2),\n",
    " #   'Guassian':GaussianMixture(n_components=n),\n",
    " #   'KMeans': KMeans(n_clusters=n) ,\n",
    "\n",
    "classifiers = {\n",
    "    'SVC':SVC(kernel=\"linear\", C=0.025),\n",
    "    'SVC2': SVC(gamma=2, C=1),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=5),\n",
    "    'RFC': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    'MLPC': MLPClassifier(alpha=1, max_iter=1000),\n",
    "    'ADABoost':AdaBoostClassifier(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'QDA':QuadraticDiscriminantAnalysis(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop over our classifiers and use the test and train datasets to generate a score to validate the classifiers. I have chosen Matthews Correlation Coefficient (MCC) which is a metric less prone to bias by taking into account false predictions, as well as true predictions. This metric lies on a score between -1 (inverse prediction) and 1 (perfect prediction) with 0 being coin toss likelihood. We then plot our results as a confusion matrix which demonstrates the predictive power of our classifiers. The Confusion matrix shows the raw number of records that have been assigned to each category in a 2x2 matrix and is given as such: \n",
    "\n",
    "|                  | Predicted Class = 0  | Predicted Class = 1 |\n",
    "|------------------|----------------------|---------------------|\n",
    "| Actual Class = 0 | True Negative        | False Positive      |\n",
    "| Actual Class = 1 |  False Negative      | True Positive       |\n",
    "\n",
    "Ideally we want the True Negative and True Positive field to be the most popular fields with only a few records in the false positive fields.\n",
    "\n",
    "N.B.: Results may vary between each run due to the stochastic nature of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "- This is not an exhaustive list of classifiers. These were mostly picked to represent all of the different kinds of models. \n",
    "Here you will find a much larger list of classifiers. Try adding some to the classifiers dictionary and see how the results differ.\n",
    "Additional models that are supported in scikit learn can be found here: https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loop through each classifier, fit training data and evaluate model. Plot results as confusion matrix##\n",
    "scores = []\n",
    "\n",
    "for i in classifiers:\n",
    "    clf_1 = classifiers.get(i)\n",
    "    clf_1.fit(X_train,y_train)\n",
    "    y_predict1 = clf_1.predict(X_test)\n",
    "    scoring = matthews_corrcoef(y_test, y_predict1)\n",
    "    scores.append(scoring) \n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_predict1)\n",
    "    ax1 = sns.heatmap(confusion_matrix, annot=True, cmap='summer')\n",
    "    title = str(i)\n",
    "    ax1.set_title(title);\n",
    "    ax1.set_xlabel('\\nPredicted Values')\n",
    "    ax1.set_ylabel('Actual Values ');\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot Performance of all Models##\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.suptitle('Performance of Machine Learning Classifiers Against Mouse and Human Antibodies', fontsize=20)\n",
    "\n",
    "bins = arange(len(classifiers))\n",
    "ax.bar(arange(len(scores)), scores)\n",
    "ax.set_ylabel('Matthews Correlation Coefficient')\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_xticks(bins)\n",
    "ax.set_xticklabels(classifiers, rotation=-80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above chart, we can see that the best performing predictors are ADA_Boost, GaussinNB, DecisionTree and SVC are the best performing classifier, wheras both Gaussian and KMeans are the worst performing with negative MCC scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Chapter Exercise: Testing our Classifiers on a Naïve Dataset\n",
    "\n",
    "We have seen that it is possible to separate mouse and human antibody protein sequences through their numerical encodings.\n",
    "\n",
    "We can also take a totally naïve dataset that the model has not been exposed to. This is a measure that checks for overfitting. If we see that there is poor performance on this naïve \"held back\" dataset, then it could suggest overfitting to the training data. Using 20 Human and 20 mouse paired sequences from OAS, which were not used to train our models, it is possible to generate their encodings, and pass them through the optimised model, in order to test it.  In this case, we will use only the top-performing models: ADABoost and GuassianNB.\n",
    "\n",
    "The file below has 20 human and 20 mouse sequences which are held back from our original training data. Using the skills you have learned so far in this notebook, encode these paired sequences and generate a list of labels for these entries. Pass them through the trained classifiers and evaluate their performance.\n",
    "\n",
    "- Comment on which classifier performs best.\n",
    "- Think of ways in which the classifiers can be improved. These will be expanded upon in the next Notebook.\n",
    "\n",
    "`naive_fasta = './Naive_dataset.faa.txt'`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-igfold",
   "language": "python",
   "name": "py38-igfold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
